{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "\n",
      " company                 object\n",
      "education               object\n",
      "experience              object\n",
      "industry                object\n",
      "jobdescription          object\n",
      "jobid                    int64\n",
      "joblocation_address     object\n",
      "jobtitle                object\n",
      "numberofpositions      float64\n",
      "payrate                 object\n",
      "postdate                object\n",
      "site_name               object\n",
      "skills                  object\n",
      "uniq_id                 object\n",
      "dtype: object\n",
      "22000\n",
      "Number of questions,columns= (22000, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Doc=====\n",
      "Backend DeveloperCandidate should have at least 4 years of experience in Micro services technologiesusing Python and Django and Deep understanding of a micro services architecture includingprofessional experience in the design build and operations of micro services in aproduction environment Experience designing REST APIs and implementing RESTfulweb services. Understanding of web services Experience in designing datapersistence system using both SQL and NoSQL, DBMS, MongoDB elastic searchGood understanding of SCRUM Agile methodology Experience in themanagement of a small team of IT professionals desirableTechnologies Stack such as Python Django/Flask Framework,Unix, GitHub, Jenkins, Kafka Kibana, Postman, JSON, Spark,AWS Deployment.Create solutions by developing, implementing,and maintaining Pythonbased components and interfaces.Define site objectives by analysing user requirements,envisioning system features and functionality.Design and develop user interfaces tointernet/intranet applications by setting expectations and features priorities throughoutdevelopment life cycle; determining design methodologies and tool sets; completingprogramming using languages and software products; designing and conducting tests.Lead the development effort of web services, Design and develop Rest based Web servicesClear understanding of web services and SOA related standards like REST/OAuth/JSON.Development using Python, Microservices, RESTful APIs and unit tests.Responsible for Design, Development, Code reviews (peer review), Unit testing,providing support to testing team, Defect fixing, Defect triaging, Root causes Analysisand release / deployment support.Identify Risks and inform the PM and others on timeMust have experience with Python and Django/Flask framework.Must have experience and knowledge in developing Microservices oriented architecture-basedservices using REST APIs.Should be comfortable in using Pyunit, Logger, Postman, Swagger.Should have hands on experience and through knowledge in understanding of data structuresand algorithms. Working experience with PostgreSQL or Elastic DB.Proficient understanding of code versioning tools such as Git, Bit Bucket.\n",
      "\n",
      "===Keywords===\n",
      "django 0.261\n",
      "python 0.237\n",
      "rest 0.233\n",
      "micro 0.225\n",
      "microservices 0.211\n",
      "understanding 0.204\n",
      "elastic 0.195\n",
      "apis 0.192\n",
      "using 0.185\n",
      "experience 0.161\n"
     ]
    }
   ],
   "source": [
    "#import all teh required models\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "# this function cleans the data by removing any special characters and digits\n",
    "def pre_process(text):\n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    #remove tags\n",
    "    text=re.sub(\"<!--?.*?-->\",\"\",text)\n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    return text\n",
    "\n",
    "# remove the stop words from the custom stop words list\n",
    "def get_stop_words(stop_file_path):\n",
    "    \"\"\"load stop words \"\"\"\n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)\n",
    "\n",
    "# sorting the data\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "# extracting the top N vectors/ words\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    return results\n",
    "\n",
    "# import the file or the sample data for training the model\n",
    "df = pd.read_csv(\"naukri_com-job_sample.csv\")\n",
    "print(\"Schema:\\n\\n\",df.dtypes)\n",
    "print(len(df))\n",
    "print(\"Number of questions,columns=\",df.shape)\n",
    "\n",
    "df['text'] = df['jobdescription'].astype(str) + df['jobtitle']\n",
    "df['text'] = df['text'].apply(lambda x:pre_process(x))\n",
    "df['text']\n",
    "\n",
    "#load a set of stop words\n",
    "stopwords=get_stop_words(\"stopwords.txt\")\n",
    "#get the text column\n",
    "docs=df['text'].tolist()\n",
    "#create a vocabulary of words, \n",
    "#ignore words that appear in 85% of documents, \n",
    "#eliminate stop words\n",
    "cv=CountVectorizer(max_df=0.85,stop_words=stopwords)\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "# print(word_count_vector)\n",
    "\n",
    "# fitting the training data\n",
    "cv=CountVectorizer(max_df=0.85,stop_words=stopwords,max_features=10000)\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "\n",
    "# displaying the vocabulary of words\n",
    "list(cv.vocabulary_.keys())\n",
    "\n",
    "#initialising the model\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "model = tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "# TODO: change the input \n",
    "# passing the test data into the model\n",
    "job_role = 'Backend Developer'\n",
    "desp = 'Candidate should have at least 4 years of experience in Micro services technologies\\\n",
    "using Python and Django and Deep understanding of a micro services architecture including\\\n",
    "professional experience in the design build and operations of micro services in a\\\n",
    "production environment Experience designing REST APIs and implementing RESTful\\\n",
    "web services. Understanding of web services Experience in designing data\\\n",
    "persistence system using both SQL and NoSQL, DBMS, MongoDB elastic search\\\n",
    "Good understanding of SCRUM Agile methodology Experience in the\\\n",
    "management of a small team of IT professionals desirable\\\n",
    "Technologies Stack such as Python Django/Flask Framework,\\\n",
    "Unix, GitHub, Jenkins, Kafka Kibana, Postman, JSON, Spark,\\\n",
    "AWS Deployment.Create solutions by developing, implementing,and maintaining Python\\\n",
    "based components and interfaces.Define site objectives by analysing user requirements,\\\n",
    "envisioning system features and functionality.Design and develop user interfaces to\\\n",
    "internet/intranet applications by setting expectations and features priorities throughout\\\n",
    "development life cycle; determining design methodologies and tool sets; completing\\\n",
    "programming using languages and software products; designing and conducting tests.\\\n",
    "Lead the development effort of web services, Design and develop Rest based Web services\\\n",
    "Clear understanding of web services and SOA related standards like REST/OAuth/JSON.\\\n",
    "Development using Python, Microservices, RESTful APIs and unit tests.\\\n",
    "Responsible for Design, Development, Code reviews (peer review), Unit testing,\\\n",
    "providing support to testing team, Defect fixing, Defect triaging, Root causes Analysis\\\n",
    "and release / deployment support.\\\n",
    "Identify Risks and inform the PM and others on time\\\n",
    "Must have experience with Python and Django/Flask framework.\\\n",
    "Must have experience and knowledge in developing Microservices oriented architecture-based\\\n",
    "services using REST APIs.Should be comfortable in using Pyunit, Logger, Postman, Swagger.\\\n",
    "Should have hands on experience and through knowledge in understanding of data structures\\\n",
    "and algorithms. Working experience with PostgreSQL or Elastic DB.\\\n",
    "Proficient understanding of code versioning tools such as Git, Bit Bucket.'\n",
    "\n",
    "# get test docs into a list\n",
    "feature_names=cv.get_feature_names()\n",
    " \n",
    "# get the document that we want to extract keywords from\n",
    "doc= job_role + desp\n",
    " \n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    " \n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    " \n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    " \n",
    "# now print the results\n",
    "print(\"\\n=====Doc=====\")\n",
    "print(doc)\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
